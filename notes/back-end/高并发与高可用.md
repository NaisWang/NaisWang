# 进程与线程

# 线程
Java语言的JVM允许程序运行多个线程，它通过java.lang.Thread类来体现。

**Thread类的特性**
- 每个线程都是通过某个特定Thread对象的run()方法来完成操作的，经常把run()方法的主体称为`线程体`
- 通过该Thread对象的start()方法来启动这个线程，start方法内部会调用run()方法。只调用run()方法，不调用start方法，是不能启动线程的
- 一个已经启动的线程不能再调用start方法，否则会抛`IllegalThreadStateException`异常

## 多线程的创建
### 方式一：通过继承Thread类
1. 创建一个继承Thread类的子类
2. 重写Thread类的run()方法
3. 创建Thread类的子类的对象
4. 通过此对象调用start()
示例：
```java
class MyThread extends Thread{
  @Override
  public void run(){
    System.out.println("jfkdf");
  }
}
public class Te{
  public static void main(String[] args) throws Exception{
    MyThread t1 = new MyThread(); 
    t1.start();//启动线程 -> 调用当前线程的run()
  }
}
```

### 方法二：通过Runnable接口
1. 创建一个实现了Runnable接口的实现类
2. 实现类去实现Runnable中的抽象方法: run()
3. 创建实现类的对象
4. 将此对象作为参数传递到Thread类的构造器中，创建Thread类的对象
5. 通过Thread类的对象调用start()
```java
class MyThread implements Runnable{
  @Override
  public void run(){
    System.out.println("fjdkf");
  }
}
public class Te<T> {
  public static void main(String[] args) throws Exception{
    Thread t1 = new Thread(new MyThread()); 
    t1.start();//启动线程 -> 调用当前线程的run() -> 调用了Runnable类型的target的run()
  }
}
```
## jdk5.0新增线程创建方式
### 新增方式一：实现Callable接口
与使用Runnable相比，Callable功能更强大些
- 相比run（）方法，可以有返回值
- 方法可以抛出异常
- 支持泛型的返回值
- 需要借助FutureTask类，比如获取返回结果
**Future接口**
- 可以对具体Rurnable、Callable任务的执行结果进行取消、查询是否完成、获取结果等。
- FutrueTask是Futrue接口的唯一的实现类
- FutureTask 同时实现了Runnable，Future接口。它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值
```java
//1. 创建一个实现Callable的实现类
class MyThread implements Callable{
  //2. 实现call方法
  @Override
  public Object call() throws Exception{
    return "whz";
  }
}
public class Te{
  public static void main(String[] args){
    //3. 创建Callable接口实现类的对象
    MyThread a = new MyThread(); 
    //4. 将此Callable接口实现类的对象作为参数传递到FutureTask构造器中，创建FutureTask对象
    FutureTask b = new FutureTask(a);
    //5. 将FutureTask的对象作为参数传递到Thread的构造器中，创建Thread对象
    Thread t = new Thread(b);
    t.run();
    try{
      //get()返回值即为FutureTask构造器参数Callable实现类重写的call()方法的返回值
      //并且get()会一直等待该返回值, 知道有返回值后才结束
      System.out.println(b.get());//输出：whz
    }catch(Exception e){

    }
  }
}
```
### 新增方式二：使用线程池
背景：经常创建和销毁、使用量特别大的资源，比如并发情况下的线程，对性能影响很大。
思路：提前创建好多个线程，放入线程池中，使用时直接获取，使用完放回池中。可以避免频繁创建销毁、实现重复利用。类似生活中的公共交通工具。
好处：
- 提高响应速度（减少了创建新线程的时间）
- 降低资源消耗（重复利用线程池中线程，不需要每次都创建）
- 便于线程管理
  - corePoolSize：核心池的大小
  - maximumPoolSize：最大线程数
  - keepAliveTime：线程没有任务时最多保持多长时间后会终止
  - ...

**JDK5.0起提供了线程池相关APl:ExecutorService和Executors**
**ExecutorService**：真正的线程池接口。常见子类ThreadPoolExecutor
- `void execute(Runnable command)`：执行任务/命令，没有返回值，一般用来执行长Runnable
- `<T>Future<T>submit(Callable<T>task)`：执行任务，有返回值，一般又来执行Callable
- `void shutdown()`：关闭连接池

**Executors**：工具类、线程池的工厂类，用于创建并返回不同类型的线程池
- Executors.newCachedThreadPool()：创建一个可根据需要创建新线程的线程池
- Executors.newFixedThreadPool(n)；创建一个可重用固定线程数的线程池
- Executors.newSingleThreadExecutor()：创建一个只有一个线程的线程池
- Executors.newScheduledThreadPool(n)：创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。

```java
class MyThread implements Callable{
  @Override
  public Object call() throws Exception{
    System.out.println(this);
    return "jfkdjf";
  }
}
class MyThread1 implements Runnable{
  @Override
  public void run(){
    System.out.println(this);
  }
}

public class Te{
  public static void main(String[] args){
    ExecutorService service = Executors.newFixedThreadPool(10);

    //设置线程池的属性
    ThreadPoolExecutor service1 = (ThreadPoolExecutor) service;
    service1.setCorePoolSize(8);

    service.execute(new MyThread1()); 
    service.execute(new MyThread1()); 
    service.submit(new MyThread());
    service.submit(new MyThread());
    service.shutdown();
  }
}
```
输出：
```
MyThread1@2b49e825
MyThread1@c0bafe7
MyThread@3b8e4865
MyThread@58381dad
```
每次执行的顺序可能会不一样

## Java多线程访问共享资源的方式：
1、如果每一个线程执行的代码相同，可以使用同一个runnable对象，这个对象中有那个共享数据（买票系统）
2、如果每一个线程执行的代码不相同，这时候需要不同的Runnable对象，有以下两种方式来实现这些Runnable对戏之间的数据共享。
(1)、将共享数据封装在另外一个对象中，然后将这个对象逐一传递给各个Runnable对象。每个线程对共享数据的操作方法也分配到那个对象身上去完成，这样容易实现针对该数据进行的各个操作的互斥和通信。
(2)、将这些Runnable对象作为某一个类中的内部类，共享数据作为这个外部类中的成员变量，每个线程对共享数据的操作方法也分配给外部类，以便实现对共享数据进行的各个操作的互斥和通信，作为内部类的各个Runnable对象调用外部类的这些方法。
(3)、上面两种方式的组合：将共享数据封装在另外一个对象中，每个线程对共享数据的操作方法也分配到那个对象身上去完成，对象作为这个外部类中的成员变量或方法中的局部变量，每个线程的Runnable对象作为外部类中的成员内部类或局部内部类。
(4)、总之，要同步互斥的几段代码最好是分别放在几个独立的方法中，这些方法再放在同一个类中，这样比较容易实现它们之间的同步互斥和通信。
3、极端且简单的方式，即在任意一个类中定义一个static的变量，这将被所有线程共享
在线程操作中由于其操作的不确定性，所以提供了一个方法，可以取得当前操作线程：
public static Thread currentThread();

## Thread类的有关方法
- `void start()`：启动线程，并执行对象的run()方法
- `run()`：线程在被调度时执行的操作
- `String getName()`：返回线程的名称
- `void setName(String name)`：设置该线程名称
- `static Thread currentThread()`：返回当前线程。
- `isAlive()`：判断当前线程是否存活
- `interrupt()`: 打断线程, 如果被打断线程正在 sleep，wait，join 会导致被打断的线程抛出 InterruptedException，并清除 打断标记 ；如果打断的正在运行的线程，则会设置 打断标记 ；park 的线程被打断，也会设置 打断标记
- `isInterrupted()`: 判断是否被打断，不会清除 打断标记
- `static interrupted()`: 判断当前线程是否被打断
- `getPriority()`: 获取线程优先级
- `setPriority(int)`: 修改线程优先级 java中规定线程优先级是1~10 的整数，较大的优先级能提高该线程被 CPU 调度的机率
- `getState()`: 获取线程状态,Java 中线程状态是用 6 个 enum 表示，分别为：NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED

**Thread类中有关的暂停和恢复的方法**
1. `sleep(long millitime)`方法
sleep()方法是Thread类中的静态方法，线程在指定的millitime时间内为阻塞状态。通常线程休眠到指定的时间后，不会立刻进入执行状态，而只是可以参与调度执行。
2. `yield()`方法
释放当前cpu的使用权，变成阻塞状态
4. `join()`方法
在线程a中调用线程b的join()，此时线程a就进入阻塞状态，直到线程b完全执行完后，线程a才结束阻塞状态
5. `wait()和notify()`方法
这两个方法是在Object类声明的方法。wait()方法使线程进入阻塞状态，直到被另一个线程唤醒。notify()方法把线程状态的变化通知并唤醒另一等待线程

## java中的线程调度
**java的调度方式：**
- 同优先级线程组成先进先出队列(先来先服务)，使用时间片策略
- 对高优先级，使用优先调度的抢占式策略

**Thread类中有关线程优先级的静态变量**
- `MAX_PRIORITY`: 10
- `MIN_PRIORITY`: 1
- `NORM_PRIORITY`: 5
java可设置的优先级数值范围为`[1,10]`

**Thread涉及的方法**
- `getPriority()`: 返回线程优先级值
- `setPriority(int newPriority)`: 改变线程的优先级

**说明**
- 线程创建时会继承父线程的优先级
- 低优先级只是获得调度的概率低，并非一定是在高优先级线程之后才被调用

## java中线程的生命周期
**JDK中用Thread.State枚举类定义了线程的六种状态**
- 初始(NEW)：新创建了一个线程对象，但还没有调用start()方法。
- 运行(RUNNABLE)：Java线程中将就绪（ready）和运行中（running）两种状态笼统的称为“运行”。线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取CPU的使用权，此时处于就绪状态（ready）。就绪状态的线程在获得CPU时间片后变为运行中状态（running）。
- 阻塞(BLOCKED)：表示线程阻塞于锁。
- 等待(WAITING)：进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断）。
- 超时等待(TIMED_WAITING)：该状态不同于WAITING，它可以在指定的时间后自行返回。
- 终止(TERMINATED)：表示该线程已经执行完毕。


![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122604.png)

### 初始状态(NEW)
实现Runnable接口和继承Thread可以得到一个线程类，new一个实例出来，线程就进入了初始状态。

### 就绪状态(RUNNABLE之READY)
- 就绪状态只是说你资格运行，调度程序没有挑选到你，你就永远是就绪状态。
- 调用线程的start()方法，此线程进入就绪状态。
- 当前线程sleep()方法结束，其他线程join()结束，等待用户输入完毕，某个线程拿到对象锁，这些线程也将进入就绪状态。
- 当前线程时间片用完了，调用当前线程的yield()方法，当前线程进入就绪状态。
- 锁池里的线程拿到对象锁后，进入就绪状态。

### 运行中状态(RUNNABLE之RUNNING)
线程调度程序从可运行池中选择一个线程作为当前线程时线程所处的状态。这也是线程进入运行状态的唯一的一种方式。

### 阻塞状态(BLOCKED)
阻塞状态是线程阻塞在进入synchronized关键字修饰的方法或代码块(获取锁)时的状态。

### 等待(WAITING)
处于这种状态的线程不会被分配CPU执行时间，它们要等待被显式地唤醒，否则会处于无限期等待的状态。

### 超时等待(TIMED_WAITING)
处于这种状态的线程不会被分配CPU执行时间，不过无须无限期等待被其他线程显示地唤醒，在达到一定时间后它们会自动唤醒。

### 终止状态(TERMINATED)
当线程的run()方法完成时，或者主线程的main()方法完成时，我们就认为它终止了。这个线程对象也许是活的，但是它已经不是一个单独执行的线程。线程一旦终止了，就不能复生。
在一个终止的线程上调用start()方法，会抛出java.lang.IllegalThreadStateException异常。

## 守护线程与非守护线程
Java分为两种线程：用户线程和守护线程

所谓**守护线程是指在程序运行的时候在后台提供一种通用服务的线程**，比如垃圾回收线程就是一个很称职的守护者，并且这种线程并不属于程序中不可或缺的部分。因 此，当所有的非守护线程结束时，程序也就终止了，同时会杀死进程中的所有守护线程。反过来说，只要任何非守护线程还在运行，程序就不会终止。

守护线程和用户线程的没啥本质的区别：唯一的不同之处就在于虚拟机的离开：如果用户线程已经全部退出运行了，只剩下守护线程存在了，虚拟机也就退出了。 因为没有了被守护者，守护线程也就没有工作可做了，也就没有继续运行程序的必要了。

将线程转换为守护线程可以通过调用Thread对象的setDaemon(true)方法来实现。在使用守护线程时需要注意一下几点：
- thread.setDaemon(true)必须在thread.start()之前设置，否则会跑出一个IllegalThreadStateException异常。你不能把正在运行的常规线程设置为守护线程。
- 在Daemon线程中产生的新线程也是Daemon的。
- 守护线程应该永远不去访问固有资源，如文件、数据库，因为它会在任何时候甚至在一个操作的中间发生中断。

## interrupt 方法详解
目标进程调用interrupt()


# 同步
## 使用synchronized
**synchronized同步代码块语法：**
```java
synchronized(同步监视器){
  //需要被同步的代码
}
```
同步监视器：也称为锁，任何一个类的实例都可以充当锁，不同地址的实例对应不同的锁。相同地址的实例对应同把锁。
需要同步的多个线程必须要使用同一把锁
```java
public class Te{
  public static void main(String[] args){
    Window w = new Window();
    Thread th1 = new Thread(w);
    Thread th2 = new Thread(w);
    th1.start();
    th2.start();
  }
}

class Window implements Runnable{
  private int ticket = 100;
  Object obj = new Object(); //当obj锁对象写在这里时，则th1与th2线程所拥有的锁相同
  @Override
  public void run(){
  //Object obj = new Object(); 当obj锁对象写在这里时，则th1与th2线程所拥有的锁不同
    while(ticket > 0){
      synchronized(obj){
        try{
          Thread.sleep(100);
        }catch(InterruptedException e){
          e.printStackTrace();
        }
        System.out.println(ticket--);
      }
    }
  }
}
```
**使用synchronized修饰方法：**
- 使用synchronized修饰非静态方法时，该方法的同步监视器为this
- 使用synchronized修饰静态方法时，该方法的同步监视器为静态方法所属类的字节码对象
```java
public class Te{
  public static void main(String[] args){
    Window w = new Window();
    Thread th1 = new Thread(w);
    Thread th2 = new Thread(w);
    th1.start();
    th2.start();
  }
}

class Window implements Runnable{
  private int ticket = 100;

  //此时同步监视器为this；
  public synchronized void reduce(){
    try{
      Thread.sleep(100);
    }catch(InterruptedException e){
      e.printStackTrace();
    }
    System.out.println(ticket--);
  }

  @Override
  public void run(){
    while(ticket > 0){
      reduce();
    }
  }
}
```
**Lock(锁)**
- 从JDK5.0开始，Java提供了更强大的线程同步机制——通过显式定义同步锁对象来实现同步。同步锁使用Lock对象充当。
- java.util.concurrent.locks.Lock接口是控制多个线程对共享资源进行访问的工具。锁提供了对共享资源的独占访问，每次只能有一个线程对Lock对象加锁，线程开始访问共享资源之前应先获得Lock对象。
- ReentrantLock 类实现了Lock，它拥有与synchronized相同的并发性和内存语义，在实现线程安全的控制中，比较常用的是ReentrantLock，可以显式加锁、释放锁。
```java
public class Te{
  public static void main(String[] args){
    Window w = new Window();
    Thread th1 = new Thread(w);
    Thread th2 = new Thread(w);
    th1.start();
    th2.start();
  }
}

class Window implements Runnable{
  private int ticket = 100;

  private ReentrantLock lock = new ReentrantLock();
  @Override
  public void run(){
    while(ticket > 0){
      try{
        //调用锁定方法
        lock.lock(); 
        Thread.sleep(100);
        System.out.println(ticket--);
      }catch(Exception e){
      }finally{
        //调用解锁方法
        lock.unlock();
      }
    }
  }
}

```
# java锁机制
## Java 对象头
锁实际上是加在对象上的，那么被加了锁的对象我们称之为锁对象，在java中，任何一个对象都能成为锁对象。而Java是通过java对象头来判断这个对象什么类型的锁对象，而Java 对象头又是什么呢?我们以 Hotspot 虚拟机为例，对象在内存中存储的布局可以分为3块域：对象头、实例数据和对齐填充。
Hopspot 对象头主要包括两部分数据：Mark Word(标记字段) 和 Klass Pointer(类型指针)。

**Mark Word**：用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为“Mark Word”。例如，在32位的HotSpot虚拟机中，如果对象处于未被锁定的状态下，那么Mark Word的32bit空间中的25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁标志位，1bit固定为0。见下图：

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122614.png)

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122625.png)

- 无状态也就是无锁的时候，对象头开辟 25bit 的空间用来存储对象的 hashcode ，4bit 用于存放分代年龄，1bit 用来存放是否偏向锁的标识位，2bit 用来存放锁标识位为01
- 偏向锁 中划分更细，还是开辟25bit 的空间，其中23bit 用来存放线程ID，2bit 用来存放 epoch，4bit 存放分代年龄，1bit 存放是否偏向锁标识， 0表示无锁，1表示偏向锁，锁的标识位还是01
- 轻量级锁中直接开辟 30bit 的空间存放指向栈中锁记录的指针，2bit 存放锁的标志位，其标志位为00
- 重量级锁中和轻量级锁一样，30bit 的空间用来存放指向重量级锁的指针，2bit 存放锁的标识位，为11
- GC标记开辟30bit 的内存空间却没有占用，2bit 空间存放锁标志位为11。
- 其中无锁和偏向锁的锁标志位都是01，只是在前面的1bit区分了这是无锁状态还是偏向锁状态。

**Klass Point**：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。

## 锁的分类及其解释
### 无锁
无锁状态，无锁即没有对资源进行锁定，所有的线程都可以对同一个资源进行访问，但是只有一个线程能够成功修改资源。

无锁的特点就是在循环内进行修改操作，线程会不断的尝试修改共享资源，直到能够成功修改资源并退出，在此过程中没有出现冲突的发生，这很像我们在之前文章中介绍的 CAS 实现，CAS 的原理和应用就是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。

### 偏向锁
Hotspot 的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，还存在锁由同一线程多次获得的情况，偏向锁就是在这种情况下出现的，它的出现是为了解决只有在一个线程执行同步时提高性能。
可以从对象头的分配中看到，偏向锁要比无锁多了线程ID 和 epoch，当一个线程访问同步代码块并获取锁时，会在对象头和栈帧的记录中存储线程的ID，等到下一次线程在进入和退出同步代码块时就不需要进行 CAS 操作进行加锁和解锁，只需要简单判断一下对象头的 Mark Word 中是否存储着指向当前线程的线程ID，判断的标志当然是根据锁的标志位来判断的。
偏向锁的获取过程
1. 访问 Mark Word 中偏向锁的标志是否设置成 1，锁的标志位是否是 01 --- 确认为可偏向状态。
2. 如果确认为可偏向状态，判断当前线程id 和锁对象的对象头中存储的线程 ID 是否一致，如果一致的话，则执行步骤5，如果不一致，进入步骤3
3. 如果当前线程ID 与对象头中存储的线程ID 不一致的话，则通过 CAS 操作来竞争获取锁。如果竞争成功，则将 Mark Word 中的线程ID 修改为当前线程ID，然后执行步骤5，如果不一致，则执行步骤4
4. 如果 CAS 获取偏向锁失败，则表示有竞争(CAS 获取偏向锁失败则表明至少有其他线程曾经获取过偏向锁，因为线程不会主动释放偏向锁)。当到达全局安全点(SafePoint)时，会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否存活(因为可能持有偏向锁的线程已经执行完毕，但是该线程并不会主动去释放偏向锁)，如果线程不处于活动状态，则将对象头置为无锁状态(标志位为01)，然后重新偏向新的线程;如果线程仍然活着，撤销偏向锁后升级到轻量级锁的状态(标志位为00)，此时轻量级锁由原持有偏向锁的线程持有，继续执行其同步代码，而正在竞争的线程会进入自旋等待获得该轻量级锁。
5. 执行同步代码

通过第4步可以看出撤销偏向锁后升级到轻量级锁的状态的开销花费还是挺大的，所以，如果某些同步代码块大多数情况下都是有两个及以上的线程竞争的话，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭

**偏向锁的释放过程**
偏向锁的释放过程可以参考上述的步骤4 ，偏向锁在遇到其他线程竞争锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点(在这个时间点上没有字节码正在执行)，它会首先暂停拥有偏向锁的线程，判断锁是否处于被锁定状态，撤销偏向锁后恢复到未锁定(标志位为01)或轻量级锁(标志位为00)的状态。

**关闭偏向锁**
偏向锁在Java 6 和Java 7 里是默认启用的。由于偏向锁是为了在只有一个线程执行同步块时提高性能，如果你确定应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking=false，那么程序默认会进入轻量级锁状态。

### 轻量级锁
轻量级锁是指当前锁是偏向锁的时候，被另外的线程所访问，那么偏向锁就会升级为轻量级锁；
升级为轻量级锁的过程：
虚拟机首先将在当前线程的栈帧中建立一个名为锁记录(Lock Record)的空间，用于存储锁对象目前的 Mark Word 的拷贝，然后拷贝对象头中的 Mark Word 复制到锁记录中。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122635.png)

拷贝成功后，虚拟机将使用 CAS 操作尝试将对象的 Mark Word 更新为指向 Lock Record 的指针，并将 Lock Record里的 owner 指针指向对象的 Mark Word。

如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为 00 ，表示此对象处于轻量级锁定状态。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122645.png)

如果这个更新操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。否则说明多个线程竞争锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为 10 ，Mark Word中存储的就是指向重量级锁(互斥量)的指针，后面等待锁的线程也要进入阻塞状态。

**轻量级锁主要有两种：自旋锁、自适应自旋锁**
**自旋锁**
所谓自旋，就是指当有另外一个线程来竞争锁时，这个线程会在原地循环等待，而不是把该线程给阻塞，直到那个获得锁的线程释放锁之后，这个线程就可以马上获得锁的。
注意，锁在原地循环的时候，是会消耗cpu的，就相当于在执行一个啥也没有的for循环。
所以，轻量级锁适用于那些同步代码块执行的很快的场景，这样，线程原地等待很短很短的时间就能够获得锁了。
经验表明，大部分同步代码块执行的时间都是很短很短的，也正是基于这个原因，才有了轻量级锁这么个东西。
自旋锁的一些问题
如果同步代码块执行的很慢，需要消耗大量的时间，那么这个时侯，其他线程在原地等待空消耗cpu，这会让人很难受。
本来一个线程把锁释放之后，当前线程是能够获得锁的，但是假如这个时候有好几个线程都在竞争这个锁的话，那么有可能当前线程会获取不到锁，还得原地等待继续空循环消耗cup，甚至有可能一直获取不到锁。
基于这个问题，我们必须给线程空循环设置一个次数，当线程超过了这个次数，我们就认为，继续使用自旋锁就不适合了，此时锁会再次膨胀，升级为重量级锁。
默认情况下，自旋的次数为10次，用户可以通过-XX:PreBlockSpin来进行更改。
自旋锁是在JDK1.4.2的时候引入的

**自适应自旋锁**
所谓自适应自旋锁就是线程空循环等待的自旋次数并非是固定的，而是会动态着根据实际情况来改变自旋等待的次数。
其大概原理是这样的：
假如一个线程1刚刚成功获得一个锁，当它把锁释放了之后，线程2获得该锁，并且线程2在运行的过程中，此时线程1又想来获得该锁了，但线程2还没有释放该锁，所以线程1只能自旋等待，但是虚拟机认为，由于线程1刚刚获得过该锁，那么虚拟机觉得线程1这次自旋也是很有可能能够再次成功获得该锁的，所以会延长线程1自旋的次数。
另外，如果对于某一个锁，一个线程自旋之后，很少成功获得该锁，那么以后这个线程要获取该锁时，是有可能直接忽略掉自旋过程，直接升级为重量级锁的，以免空循环等待浪费资源。
轻量级锁也被称为非阻塞同步、乐观锁，因为这个过程并没有把线程阻塞挂起，而是让线程空循环等待，串行执行。

### 重量级锁
锁标识位为10。该锁对象的对象头的MarkWord去尝试指向操作系统的Monitor对象，让锁对象中的MarkWord和Monitor对象相关联。如果关联成功，将锁对象中的锁标识位为设为10，且Monitor的Owner指向该线程。

下面描述获取重量级锁的过程（假设下面的obj锁对象已经是重量级锁了）:
当Thread1访问到synchronized(obj)中的共享资源的时候:
- 首先会将synchronized中的锁对象中对象头的MarkWord去尝试指向操作系统的Monitor对象, 让锁对象中的MarkWord和Monitor对象相关联. 如果关联成功（注：锁对象不同，就会指向不同的Monitor）, 将obj对象头中的MarkWord的对象状态改为10。 Monitor的Owner将指向Thread1
- 又来了个Thread2执行synchronized(obj)代码, 它首先会看看能不能执行该临界区的代码; 它会检查obj是否关联了Montior, 此时已经有关联了, 它就会去看看该Montior的Owner有没有指向，发现Owner指向Thread1; 此时该Thread2就会进入到它的EntryList(阻塞队列);
- 当Thread1执行完临界区代码后, Monitor的Owner(所有者)就空出来了. 此时就会通知Monitor中的EntryList阻塞队列中的线程, 这些线程通过竞争, 成为新的所有者

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122655.png)

由于Monitor锁本质又是依赖于底层的操作系统的 Mutex Lock(互斥锁)来实现的。而操作系统实现线程之间的切换需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么 Synchronized 效率低的原因。因此，这种依赖于操作系统 Mutex Lock 所实现的锁我们称之为重量级锁。

**注：synchronized关键字并非一开始就该对象加上重量级锁，也是从偏向锁，轻量级锁，再到重量级锁的过程。**

# 乐观锁与悲观锁
乐观锁和悲观锁是两种<font color="red">思想</font>，它们的使用是非常广泛的，不局限于某种编程语言或数据库。
## 乐观锁
乐观锁：乐观锁在操作数据时非常乐观，认为别人不会同时修改数据。因此乐观锁不会上锁，只是在执行更新的时候判断一下在此期间别人是否修改了数据：如果别人修改了数据则放弃操作，否则执行操作。

乐观锁的实现方式主要有两种：CAS机制和版本号机制

**版本号机制实现乐观锁**
版本号机制的基本思路是在数据中增加一个字段version，表示该数据的版本号，每当数据被修改，版本号加1。当某个线程查询数据时，将该数据的版本号一起查出来；当该线程更新数据时，判断当前版本号与之前读取的版本号是否一致，如果一致才进行操作。
需要注意的是，这里使用了版本号作为判断数据变化的标记，实际上可以根据实际情况选用其他能够标记数据版本的字段，如时间戳等。

**乐观锁加锁吗？**
- 乐观锁本身是不加锁的，只是在更新时判断一下数据是否被其他线程更新了；AtomicInteger便是一个例子。
- 有时乐观锁可能与加锁操作合作，但不能改变“乐观锁本身不加锁”这一事实

## 悲观锁
悲观锁：悲观锁在操作数据时比较悲观，认为别人会同时修改数据。因此操作数据时直接把数据锁住，直到操作完成后才会释放锁；上锁期间其他人不能修改数据。

悲观锁的实现方式是加锁，加锁既可以是对代码块加锁（如Java的synchronized关键字），也可以是对数据加锁（如MySQL中的排它锁）。

## 优缺点和适用场景
乐观锁和悲观锁并没有优劣之分，它们有各自适合的场景；下面从两个方面进行说明。
**1、功能限制**
与悲观锁相比，乐观锁适用的场景受到了更多的限制，无论是CAS还是版本号机制。
例如，CAS只能保证单个变量操作的原子性，当涉及到多个变量时，CAS是无能为力的，而synchronized则可以通过对整个代码块加锁来处理。再比如版本号机制，如果query的时候是针对表1，而update的时候是针对表2，也很难通过简单的版本号来实现乐观锁。

**2、竞争激烈程度**
如果悲观锁和乐观锁都可以使用，那么选择就要考虑竞争的激烈程度：
当竞争不激烈 (出现并发冲突的概率小)时，乐观锁更有优势，因为悲观锁会锁住代码块或数据，其他线程无法同时访问，影响并发，而且加锁和释放锁都需要消耗额外的资源。
当竞争激烈(出现并发冲突的概率大)时，悲观锁更有优势，因为乐观锁在执行更新时频繁失败，需要不断重试，浪费CPU资源。

# CAS
什么是CAS(Compare And Swap)? 即比较并替换，实现并发算法时常用到的一种技术。CAS操作包含三个操作数：
- 需要读写的内存位置(V)
- 进行比较的预期值(A)
- 拟写入的新值(B)
CAS操作逻辑如下：如果内存位置V的值等于预期的A值，则将该位置更新为新值B，否则不进行任何操作。许多CAS的操作是`自旋`的：如果操作不成功，会一直重试，直到操作成功为止。

这里引出一个新的问题，既然CAS包含了Compare和Swap两个操作，它又如何保证原子性呢？答案是：**CAS是由CPU支持的原子操作，其原子性是在硬件层面进行保证的。**

如下源代码释义所示，这部分主要为CAS相关操作的方法。
```java
/**
	*  CAS
  * @param o         包含要修改field的对象
  * @param offset    对象中某field的偏移量
  * @param expected  期望值
  * @param update    更新值
  * @return          true | false
  */
public final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);

public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);
  
public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);
```

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122708.png)

下面以Java中的自增操作(i++)为例，看一下悲观锁和CAS分别是如何保证线程安全的。我们知道，在Java中自增操作不是原子操作，它实际上包含三个独立的操作：（1）读取i值；（2）加1；（3）将新值写回i

因此，如果并发执行自增操作，可能导致计算结果的不准确。在下面的代码示例中：value1没有进行任何线程安全方面的保护，value2使用了乐观锁(CAS)，value3使用了悲观锁(synchronized)。运行程序，使用1000个线程同时对value1、value2和value3进行自增操作，可以发现：value2和value3的值总是等于1000，而value1的值常常小于1000。
```java
public class Test {
  //value1：线程不安全
  private static int value1 = 0;
  //value2：使用乐观锁
  private static AtomicInteger value2 = new AtomicInteger(0);
  //value3：使用悲观锁
  private static int value3 = 0;
  private static synchronized void increaseValue3(){
    value3++;
  }
    
  public static void main(String[] args) throws Exception {
    //开启1000个线程，并执行自增操作
    for(int i = 0; i < 1000; ++i){
      new Thread(new Runnable() {
        @Override
        public void run() {
          try {
            Thread.sleep(100);
          } catch (InterruptedException e) {
            e.printStackTrace();
          }
          value1++;
          value2.getAndIncrement();
          increaseValue3();
        }
      }).start();
    }
    //打印结果
    Thread.sleep(1000);
    System.out.println("线程不安全：" + value1);
    System.out.println("乐观锁(AtomicInteger)：" + value2);
    System.out.println("悲观锁(synchronized)：" + value3);
  }
}
```
首先来介绍AtomicInteger。AtomicInteger是java.util.concurrent.atomic包提供的原子类，利用CPU提供的CAS操作来保证原子性；除了AtomicInteger外，还有AtomicBoolean、AtomicLong、AtomicReference等众多原子类。

下面看一下AtomicInteger的源码，了解下它的自增操作getAndIncrement()是如何实现的（源码以Java7为例，Java8有所不同，但思想类似）。
```java
public class AtomicInteger extends Number implements java.io.Serializable {
  //存储整数值，volatile保证可视性
  private volatile int value;
  //Unsafe用于实现对底层资源的访问
  private static final Unsafe unsafe = Unsafe.getUnsafe();

  //valueOffset是value在内存中的偏移量
  private static final long valueOffset;
  //通过Unsafe获得valueOffset
  static {
    try {
      valueOffset = unsafe.objectFieldOffset(AtomicInteger.class.getDeclaredField("value"));
    } catch (Exception ex) { throw new Error(ex); }
  }

  public final boolean compareAndSet(int expect, int update) {
    return unsafe.compareAndSwapInt(this, valueOffset, expect, update);
  }

  public final int getAndIncrement() {
    for (;;) {
      int current = get();
      int next = current + 1;
      if (compareAndSet(current, next))
        return current;
    }
  }
}
```
源码分析说明如下：
- getAndIncrement()实现的自增操作是自旋CAS操作：在循环中进行compareAndSet，如果执行成功则退出，否则一直执行。
- 其中compareAndSet是CAS操作的核心，它是利用Unsafe对象实现的。
- Unsafe又是何许人也呢？Unsafe是用来帮助Java访问操作系统底层资源的类（如可以分配内存、释放内存），通过Unsafe，Java具有了底层操作能力，可以提升运行效率；强大的底层资源操作能力也带来了安全隐患(类的名字Unsafe也在提醒我们这一点)，因此正常情况下用户无法使用。AtomicInteger在这里使用了Unsafe提供的CAS功能。
- valueOffset可以理解为value在内存中的偏移量，对应了CAS三个操作数(V/A/B)中的V；偏移量的获得也是通过Unsafe实现的。
- value域的volatile修饰符：Java并发编程要保证线程安全，需要保证原子性、可视性和有序性；CAS操作可以保证原子性，而volatile可以保证可视性和一定程度的有序性；在AtomicInteger中，volatile和CAS一起保证了线程安全性。

说完了AtomicInteger，再说synchronized。synchronized通过对代码块加锁来保证线程安全：在同一时刻，只能有一个线程可以执行代码块中的代码。synchronized是一个重量级的操作，不仅是因为加锁需要消耗额外的资源，还因为线程状态的切换会涉及操作系统核心态和用户态的转换；不过随着JVM对锁进行的一系列优化(如自旋锁、轻量级锁、锁粗化等)，synchronized的性能表现已经越来越好。

## CAS的缺点：
### ABA问题
在多线程场景下CAS会出现ABA问题，关于ABA问题这里简单科普下，例如有2个线程同时对同一个值(初始值为A)进行CAS操作，这二个线程如下
- 线程1，期望值为A，欲更新的值为B
- 线程2，期望值为A，欲更新的值为B
线程1抢先获得CPU时间片，而线程2因为其他原因阻塞了，线程1取值与期望的A值比较，发现相等然后将值更新为B，然后这个时候出现了线程3，期望值为B，欲更新的值为A，线程3取值与期望的值B比较，发现相等则将值更新为A，此时线程2从阻塞中恢复，并且获得了CPU时间片，这时候线程2取值与期望的值A比较，发现相等则将值更新为B，虽然线程2也完成了操作，但是线程2并不知道值已经经过了A->B->A的变化过程。

**ABA问题带来的危害：**
小明在提款机，提取了50元，因为提款机问题，有两个线程，同时把余额从100变为50
线程1（提款机）：获取当前值100，期望更新为50，
线程2（提款机）：获取当前值100，期望更新为50，
线程1成功执行，线程2某种原因block了，这时，某人给小明汇款50
线程3（默认）：获取当前值50，期望更新为100，
这时候线程3成功执行，余额变为100，
线程2从Block中恢复，获取到的也是100，compare之后，继续更新余额为50！！！
此时可以看到，实际余额应该为100（100-50+50），但是实际上变为了50（100-50+50-50）这就是ABA问题带来的成功提交。

**解决方法：**
在变量前面加上版本号，每次变量更新的时候变量的版本号都+1，即A->B->A就变成了1A->2B->3A。

### 循环时间长开销大
在并发冲突概率大的高竞争环境下，如果CAS一直失败，会一直重试，CPU开销较大。针对这个问题的一个思路是引入退出机制，如重试次数超过一定阈值后失败退出。当然，更重要的是避免在高竞争环境下使用乐观锁。

**解决方法：**
限制自旋次数，防止进入死循环。
只能保证一个共享变量的原子操作

### 功能限制
CAS的功能是比较受限的，例如CAS只能保证单个变量（或者说单个内存值）操作的原子性，这意味着：(1)原子性不一定能保证线程安全，例如在Java中需要与volatile配合来保证线程安全；(2)当涉及到多个变量(内存值)时，CAS也无能为力。

**解决方法：**
如果需要对多个共享变量进行操作，可以使用加锁方式(悲观锁)保证原子性，或者可以把多个共享变量合并成一个共享变量进行CAS操作。


# 公平锁与非公平锁
**公平锁：**多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。
- 优点：所有的线程都能得到资源，不会饿死在队列中。
- 缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大。

**非公平锁**：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。
- 优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。
- 缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。

## synchronized非公平锁
```java
public class SyncUnFairLockTest {
  //食堂
  private static class DiningRoom {
    //获取食物
    public void getFood() {
      System.out.println(Thread.currentThread().getName()+":排队中");
      synchronized (this) {
        System.out.println(Thread.currentThread().getName()+":@@@@@@打饭中@@@@@@@");
      }
    }
  }

  public static void main(String[] args) {
    DiningRoom diningRoom = new DiningRoom();
    //让5个同学去打饭
    for (int i=0; i<5; i++) {
      new Thread(()->{
          diningRoom.getFood();
      },"同学编号:00"+(i+1)).start();
    }
  }
}
```
如上代码：我们定义一个内部类DiningRoom表示食堂，getFood方法里面用synchronized锁修饰this指向DiningRoom的实例对象（22行中的diningRoom对象），主类中让编号001至005五个同学同时去打饭，用于测试先排队的同学是否能先打到饭？运行程序得到其中一种执行结果如下图所示，002->004->001->003->005同学先去排队，但打饭的顺序是002->003->001->004->005，说明这里003和001两个同学插队了，插到004前面了，我们详细分析执行过程，002先抢到锁打饭了，释放了锁，本来应该是接下来是004抢到锁去打饭（因为004是比003先来排队），但003抢到锁，打饭了，释放了锁，这是第一次插队。现在还是来004抢锁，但是没抢到又被001抢到了，释放锁后才被004抢到，这是第二次插队，后面分别再是004->005抢到锁，释放锁，程序执行完毕。因为003和001插队，我们用代码证明了synchronized是非公平锁。紧接着我们来看下ReentrantLock公平锁和非公平锁。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122717.png)

## ReentrantLock非公平锁
```java
public class UnFairLockTest {

  private static final Lock LOCK = new ReentrantLock(false);

  //食堂
  private static class DiningRoom {
    //获取食物
    public void getFood() {
      try {
        System.out.println(Thread.currentThread().getName()+":正在排队");
        LOCK.lock();
        System.out.println(Thread.currentThread().getName()+":@@@@@@打饭中@@@@@@@");
      } catch (Exception e) {
        e.printStackTrace();
      } finally {
        LOCK.unlock();
      }
    }
  }

  public static void main(String[] args) throws InterruptedException {
    DiningRoom diningRoom = new DiningRoom();
    //让5个同学去打饭
    for (int i=0; i<5; i++) {
      new Thread(()->{
          diningRoom.getFood();
      },"同学编号:00"+(i+1)).start();
    }
  }
}
```
如上代码：我们在代码第13行中定义了Lock LOCK = new ReentrantLock(false)；ReentrantLock的参数是false表示非公平锁，上面代码需要用LOCK.lock()加锁，LOCK.unlock()解锁，需要放入try，finally代码块中，目的是如果try中加锁后代码发生异常锁最终执行LOCK.unlock()，锁总能被释放。主类中让编号001至005五个同学同时去打饭，得到其中一种执行结果如下图所示，001->004->005->003->002同学先去排队，但打饭的顺序是001->005->004->003->002，这里005同学插队了，插到004前面。我们详细分析执行过程：001先来抢到锁打饭了并释放了锁，接下来本应该是004抢到锁，因为它先排队，但005却在004之前抢到锁，打饭了，005比004后来，却先打饭，这就是不公平锁，后面的执行结果按先来后到执行，程序结束。我们用代码证明了ReentrantLock是非公平的锁。紧接着我们来看下ReentrantLock另一种作为公平锁的情况。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122727.png)

## ReentrantLock公平锁
基于上面的案例，我们不重复贴代码了，将上述代码中13行的private static final Lock LOCK = new ReentrantLock(false)；参数由false改为true，private static final Lock LOCK = new ReentrantLock(true)；无论执行多少次可以得出一个结论：先排队的童鞋能先打饭，不允许插队体现的就是公平锁。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122739.png)

# 硬件中的并发问题
<font color="red">一定要注意，我们是面向jvm编程的，而不是面向硬件编程的。jvm帮我们做了很多的处理，所以我们只需要按照jvm的规定来实现并发编程即可。</font>
## Cache
为了更清楚的硬件并发问题，我们先看一下Cache与CacheLine
**Cache**
cache，中译名高速缓冲存储器，其作用是为了更好的利用局部性原理，减少CPU访问主存的次数。简单地说，CPU正在访问的指令和数据，其可能会被以后多次访问到，或者是该指令和数据附近的内存区域，也可能会被多次访问。因此，第一次访问这一块区域时，将其复制到cache中，以后访问该区域的指令或者数据时，就不用再从主存中取出。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122749.png)
cache分成多个组，每个组分成多个行，linesize是cache的基本单位，从主存向cache迁移数据都是按照linesize为单位替换的。比如linesize为32Byte，那么迁移必须一次迁移32Byte到cache

当cpu访问一个数据时，先是拿的主存地址，cpu会先拿着这个主存地址到cache中找，若cache命中，就将主存地址转换为cache地址，直接对cache进行操作，与主存无关；若cache不命中(即**cache miss**)，则仍需要访问主存。
根据命中方式，我们可以将cache分为3类：
- 直接映射高速缓存
- 组相联高速缓存
- 全相联高速缓存
具体这三种方式是如何实现的，可以看计组

**Cache 结构**
假设内存容量为M，内存地址为m位：那么寻址范围为000…00~FFF…F(m位)
倘若把内存地址分为以下三个区间：

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122758.png)

tag, set index, block offset三个区间有什么用呢？再来看看Cache的逻辑结构吧

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122807.png)

将此图与上图做对比，可以得出各参数如下：
B = 2^b
S = 2^s
现在来解释一下各个参数的意义：
一个cache被分为S个组，每个组有E个cacheline，而一个cacheline中，有B个存储单元，现代处理器中，这个存储单元一般是以字节(通常8个位)为单位的，也是最小的寻址单元。因此，在一个内存地址中，中间的s位决定了该单元被映射到哪一组，而最低的b位决定了该单元在cacheline中的偏移量。valid通常是一位，代表该cacheline是否是有效的(当该cacheline不存在内存映射时，当然是无效的)。tag就是内存地址的高t位，因为可能会有多个内存地址映射到同一个cacheline中，所以该位是用来校验该cacheline是否是CPU要访问的内存单元。

**Cache Line**
当从内存中取单元到cache中时，会一次取一个cacheline大小的内存区域到cache中，然后存进相应的cacheline中。

## 缓存不一致
现代处理器为了提高内存数据的访问速度，都会有使用cache，**缓存的存在极大的加快了处理器访问内存的速度。但事情总是有两面性的，缓存的存在加快了堆内存的访问速度，同时也带来了一系列额外的复杂性。每个 CPU 缓存中有一份自己的内存副本，会带来各个 CPU 在访问同一块内存的数据时，每个 CPU 缓存中的副本可能不一致的问题**。

**缓存一致性协议**用于管理多个 CPU cache 之间数据的一致性，这些协议十分复杂，<font color="red">不同的cpu，不同的硬件，可能采用的缓存一致性协议不同</font>，在这里我们仅讨论 MESI 协议；
**MESI协议**，其主要表示缓存数据有四个状态：Modified（已修改）, Exclusive（独占的）,Shared（共享的），Invalid（无效的）。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122819.png)

协议在每一个 cache line 中维护一个两位的状态 “tag” ，这个 “tag” 在cache line的物理地址或者数据后。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122832.png)

**MESI 协议消息**
- Read。"read" 消息用来获取指定的数据。会到其他cpu cache或内存中找。 
- Read Response。该消息携带了 “read” 消息所请求的数据。read response可能来自于内存或者是其他CPU cache。
- Invalidate。当前cpu想要写修改cache中数据时，且这个数据状态为shared时，才会向其他cache发送该信息，其他 CPU cache 在收到该消息后，必须进行匹配，发现在自己的cache line中有该地址的数据，那么就将其数据状态设置为Invalid，并响应 Invalidate Acknowledge 回应来告诉它我已经设置完了，只有当收到这个响应，才能将数据状态设置成Exclusive。当修改完后数据后，数据状态才变为Modified。如果不是share状态，则直接进行修改，无法发送Invalidate消息。 
- Invalidate Acknowledge。该消息用做回应 Invalidate 消息。
- Read Invalidate。该消息中带有物理地址，用来说明想要获取的数据。这个消息还有 Invalidate 消息的效果。其实该消息是 read + Invalidate 消息的组合，发送该消息后cache期望收到一个 read response 回应与Invalidate Acknowledge回应。
- Writeback。 该消息带有地址和数据，该消息用在 modified 状态的 cache line 被置换时发出，用来将最新的数据写回 memory 或其他下一级 cache 中。

## 内存屏障（Memory Barrier）
问题的产生

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122842.png)
如上图 CPU 0 执行了一次写操作。于是 CPU 0 发送了一个 Invalidate 消息，其他所有的 CPU 在收到这个 Invalidate 消息之后，需要将自己 CPU local cache 中的该数据从 cache 中清除，并且发送消息acknowledge告知 CPU 0。CPU 0 在收到所有 CPU 发送的 ack 消息后会将数据写入到自己的local cache中。这里就产生了性能问题：当 CPU 0 在等待其他 CPU 的 ack 消息时是处于停滞的（stall）状态，大部分的时间都是在等待消息。为了提高性能就引入的 Store Buffer。

**Store Buffer**

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122852.png)

store buffer 的目的是让 CPU 不再操作之前进行漫长的等待时间，而是将数据先写入到store buffer中，CPU 无需等待可以继续执行其他指令，等到 CPU 收到了 ack 消息后，再从 store buffer 中将数据写入到 local cache 中。有了 store buffer 之后性能提高了许多，但常言道：“有一利必有一弊。”store buffer 虽然提高了性能但是却引入了新的问题。如下：

```java
//CPU 0执行一下代码，a在CPU 1的cache上，初始值为0
a = 1;
b = a + 1;
assert(b == 2);
```
上述代码的执行序列如下：
1. cpu 0 执行a=1时，由于cpu 0的cache 中没有a ，发生cache miss。 会先发送read invalidate 信息。然后把a等于1的结果放到store buffer上。
2. CPU 1 收到了 read invalidate 消息，回应 read response和acknowledge消息，把自己local cache中的a清除了。此时cpu 0收到read response回应，则cpu0的cache上有a，且值为0，假设此时cpu 0还没有收到acknowledge回应，所以store buffer中的数据还没有写入到cache中
3. CPU 0 执行 b = a+1 。从cache上获取到a的值为0，执行a+1,赋给b，此时b 被 CPU 0 独占所以直接写入 cache line ， 这时候 b 的值为 1。
4. CPU 0 收到acknowledge响应后，将 store buffer中a的值写入到 cache line ， a变为1。
5. CPU 0 执行 assert(b == 2) , 断言失败

导致这个问题是因为 CPU 对内存进行操作的时候，顺序和程序代码指令顺序不一致。在写操作执行之前就先执行了读操作。另一个原因是在同一个 CPU 中同一个数据存在不一致的情况 ， 在 store buffer 中是最新的数据， 在 cache line 中是旧的数据。为了解决在同一个 CPU 的 store buffer 和 cache 之间数据不一致的问题，引入了 Store Forwarding。store forwarding 就是当 CPU 执行读操作时，会从 store buffer 和 cache 中读取数据， 如果 store buffer 中有数据会使用 store buffer 中的数据，这样就解决了同一个 CPU 中数据不一致的问题。**但是一个cpu是只能访问另一个cpu中的cache，而不能访问到另一个cpu中的store buffer的**，所以依然会引入新的问题，如下：
```java
//CPU 0执行foo(), 拥有b的Cache Line, 初始值是0
void foo(){
  a = 1;
  b = 1;
}
//CPU 1执行bar(), 拥有aCache Line, 初始值是0
void bar(){
  while(b == 0) continue;
  assert(a == 1);
}
```
执行流程：
1. CPU 0执行a=1的赋值操作，由于a不在cache中，因此，CPU 0将a值放到store buffer中之后，发送了read invalidate命令到总线上去。
2. CPU 1执行 while (b == 0) 循环，由于b不在CPU 1的cache中，因此，CPU发送一个read message到总线上，看看是否可以从其他cpu的cache中或者memory中获取数据。
3. CPU 0继续执行b=1的赋值语句，由于b就在自己的local cache中（cacheline处于modified状态或者exclusive状态），因此CPU0可以直接操作将新的值1写入cache line。
4. CPU 0收到了read message，将最新的b值”1“回送给CPU 1，同时将b cacheline的状态设定为shared。
5. CPU 1收到了来自CPU 0的read response消息，将b变量的最新值”1“值写入自己的cacheline，状态修改为shared。
6. 由于b值等于1了，因此CPU 1跳出while (b == 0)的循环，继续执行。
7. CPU 1执行assert(a == 1)，这时候CPU 1的local cache中还是旧的a值，因此assert(a == 1)失败。
8. CPU 1收到了来自CPU 0的read invalidate消息，以a变量的值进行回应，同时清空自己的cacheline。
9. CPU 0收到了read response和invalidate ack的消息之后，将store buffer中的a的最新值”1“数据写入cacheline。

产生问题的原因是 CPU 0 对 a 的写操作还没有执行完，但是 CPU 1 对 a 的读操作已经执行了。毕竟CPU并不知道哪些变量有相关性，这些变量是如何相关的。不过CPU设计者可以间接提供一些工具让软件工程师来控制这些相关性。这些工具就是**memory barrier 指令**。要想程序正常运行，必须增加一些 memory barrier 的操作。

### Store Memory Barrier（写内存屏障）
```java
//CPU 0执行foo(), 拥有b的Cache Line, 初始值是0
void fun1() {   
  a = 1;   
  smp_mb();   
  b = 1;
}
 
//CPU 1执行fun2(), 拥有aCache Line, 初始值是0
void fun2() {   
  while (b == 0) continue;
  assert(a == 1);
}
``` 

smp_mb() 这个内存屏障的操作会在执行后续的store操作之前，首先flush store buffer（也就是将之前的值写入到cacheline中）。smp_mb() 操作主要是为了让数据在local cache中的操作顺序是符合program order的顺序的，为了达到这个目标有两种方法：方法一就是让CPU stall，直到完成了清空了store buffer（也就是把store buffer中的数据写入cacheline了）。方法二是让CPU可以继续运行，不过需要在store buffer中做些文章，也就是要记录store buffer中数据的顺序，在将store buffer的数据更新到cacheline的操作中，严格按照顺序执行，即便是后来的store buffer数据对应的cacheline已经ready，也不能执行操作，要等前面的store buffer值写到cacheline之后才操作。增加smp_mb() 之后，操作顺序如下：
1. CPU 0执行a=1的赋值操作，由于a不在local cache中，因此，CPU 0将a值放 store buffer中之后，发送了read invalidate命令到总线上去。
2. CPU 1执行 while (b == 0) 循环，由于b不在CPU 1的cache中，因此，CPU发送一个read message到总线上，看看是否可以从其他cpu的local cache中或者memory中获取数据。
3. CPU 0执行smp_mb()函数，给目前store buffer中的所有项做一个标记（后面我们称之marked entries）。当然，针对我们这个例子，store buffer中只有一个marked entry就是“a=1”。
4. CPU 0继续执行b=1的赋值语句，虽然b就在自己的local cache中（cacheline处于modified状态或者exclusive状态），不过在store buffer中有marked entry，因此CPU0并没有直接操作将新的值1写入cache line，取而代之是b的新值”1“被写入store buffer，当然是unmarked状态。
5. CPU 0收到了read message，将b值”0“（新值”1“还在store buffer中）回送给CPU 1，同时将b cacheline的状态设定为shared。
6.  CPU 1收到了来自CPU 0的read response消息，将b变量的值（”0“）写入自己的cacheline，状态修改为shared。
7. 完成了bus transaction之后，CPU 1可以load b到寄存器中了（local cacheline中已经有b值了），当然，这时候b仍然等于0，因此循环不断的loop。虽然b值在CPU 0上已经赋值等于1，但是那个新值被安全的隐藏在CPU 0的store buffer中。
8. CPU 1收到了来自CPU 0的read invalidate消息，以a变量的值进行回应，同时清空自己的cacheline。
9. CPU 0将store buffer中的a值写入cacheline，并且将cacheline状态修改为modified状态。
10. 由于store buffer只有一项marked entry（对应a=1），因此，完成step 9之后，store buffer的b也可以进入cacheline了。不过需要注意的是，当前b对应的cacheline的状态是shared。
11.  CPU 0发送invalidate消息，请求b数据的独占权。
12.  CPU 1收到invalidate消息，清空自己的b cacheline，并回送acknowledgement给CPU 0。
13. CPU 1继续执行while (b == 0)，由于b不在自己的local cache中，因此 CPU 1发送read消息，请求获取b的数据。
14. CPU 0收到acknowledgement消息，将b对应的cacheline修改成exclusive状态，这时候，CPU 0终于可以将b的新值1写入cacheline。
15. CPU 0收到read消息，将b的新值1回送给CPU 1，同时将其local cache中b对应的cacheline状态修改为shared。
16.  CPU 1获取来自CPU 0的b的新值，将其放入cacheline中。
17. 由于b值等于1了，因此CPU 1跳出while (b == 0)的循环，继续执行。
18. CPU 1执行assert(a == 1)，不过这时候a值没有在自己的cacheline中，因此需要通过cache一致性协议从CPU 0那里获得，这时候获取的是a的最新值，也就是1值，因此assert成功。

通过上面的描述，我们可以看到，一个直观上很简单的给a变量赋值的操作，都需要那么长的执行过程，而且每一步都需要芯片参与，最终完成整个复杂的赋值操作过程。

**上述这个例子展示了 write memory barrier , 简单来说在屏障之后的写操作必须等待屏障之前的写操作完成才可以执行，读操作则不受该屏障的影响。**

**读内存屏障 Load Memory Barrier就不详细说明了**

**总结**
**Write Barrier**
Store/Write屏障，是x86的”sfence“指令，屏障之后的写操作必须等待屏障之前的写操作完成才可以执行，读操作则不受该屏障的影响

**Load Barrier**
Load屏障，是x86上的”ifence“指令，屏障之后的读操作必须等待屏障之前的读操作完成才可以执行，写操作则不受该屏障的影响

**Full Barrier**
Full屏障，是x86上的”mfence“指令，复合了load和write屏障的功能。


# Java 内存模型（Java Memory Model）
上述我们了解了硬件底层并发问题，上述只是针对某个cpu架构来谈并发问题的，针对不同的cpu架构，所产生的并发问题是不同的。所以Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）**来屏蔽掉各种硬件和操作系统的内存访问差异（所以在写java代码时，<font color="red">应该要严格面向jvm，而不是面向底层硬件</font>）**，以实现让Java程序在各种平台下都能达到一致的内存访问效果。在此之前，主流程序语言（如C/C++等）直接使用物理硬件和操作系统的内存模型，因此，会由于不同平台上内存模型的差异，有可能导致程序在一套平台上并发完全正常，而在另外一套平台上并发访问却经常出错，因此在某些场景就必须针对不同的平台来编写程序。

**java内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节**。此处的变量（Variables）与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但不包括局部变量与方法参数，因为后者是线程私有的，不会被共享，自然就不会存在竞争问题。为了获得较好的执行效能，Java内存模型并没有限制执行引擎使用处理器的特定寄存器或缓存来和主内存进行交互，也没有限制即时编译器进行调整代码执行顺序这类优化措施。
Java内存模型规定了所有的变量都存储在主内存（Main Memory）中。每条线程还有自己的工作内存（Working Memory），线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝（不是全部拷贝），线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的变量。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。线程、主内存、工作内存三者的交互关系如下：

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122903.png)

- read 读取，作用于主内存把变量从主内存中读取到本本地内存。
- load 加载，主要作用本地内存，把从主内存中读取的变量加载到本地内存的变量副本中
- use 使用，主要作用本地内存，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。、
- assign 赋值 作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。
- store 存储 作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。
- write 写入 作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。
- lock 锁定 ：作用于主内存的变量，把一个变量标识为一条线程独占状态。
- unlock 解锁：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。
所以看似简单的通信其实是这八种状态来实现的。
同时在Java内存模型中明确规定了要执行这些操作需要满足以下规则：
- 不允许read和load、store和write的操作单独出现。
- 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。
- 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。
- 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。
- 一个变量在同一时刻只允许一条线程对其进行lock操作，lock和unlock必须成对出现
- 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值
- 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。
- 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。

这里所讲的主内存、工作内存与Java内存区域中的Java堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的，如果两者一定要勉强对应起来，那从变量、主内存、工作内存的定义来看，主内存主要对应于java堆中的对象实例数据部分，而工作内存则对应于虚拟机栈中的部分区域。从更低层次上说，主内存就直接对应于物理硬件的内存，而为了获取更好的运行速度，虚拟机（甚至是硬件系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问读写的是工作内存

**JMM模型下的线程间通信：**
线程间通信必须要经过主内存。
如下，如果线程A与线程B之间要通信的话，必须要经历下面2个步骤：
- 线程A把本地内存A中更新过的共享变量刷新到主内存中去。
- 线程B到主内存中去读取线程A之前已更新过的共享变量。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122914.png)

**JMM 是一套规则呀，它只会给你定义规范，模型，具体的实现是由JVM来解决的！理解这一点很重要。**

## volatile
### 实现变量可见性
可见性是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。也就是一个线程修改的结果，另一个线程马上就能看到。
当一个变量定义为volatile之后，当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。而普通变量不能做到这一点，普通变量的值在线程间传递均需要通过主内存来完成，例如，线程A修改一个普通变量的值，然后向主内存进行回写，另外一条线程B在线程A回写完成了之后再从主内存进行读取操作，新变量值才会对线程B可见。

**valatile的底层实现**
底层实现主要是通过`汇编lock前缀指令`，这个汇编指令会锁定这块内存区域的缓存(缓存行锁定)并**立即**回写到主内存
IA-32和Intel 64架构软件开发者手册对lock指令的解释：
- 会将当前处理器缓存行(cache line)的数据<font color="red">立即</font>写回到内存中
- 这个写会内存的操作会引起在其他cpu里缓存了该内存地址的数据无效(类似MESI协议) 
- 提供内存屏障功能，是lock前后指令不能重排序

### 非原子性
注：**由于java里面的运算并非原子操作，导致volatile变量的运算在并发下一样是不安全的**，我们可以通过下面代码验证：
```java
public class VolatileTest{

  public static volatile int race = 0;

  public static void increase(){
    race++;
  }

  public static void main(String[] args){
    for(int i = 0; i < 20; i++){
      Thread th = new Thread(() -> {
        for(int j = 0; j < 10000; j++){
          increase();
        }
      });
      th.start();
    }

    while(Thread.activeCount() > 1){
      Thread.yield();
    }

    System.out.println(race);
  }
}
```
这段代码发起了20个线程，每个线程对race变量进行1000次自增操作，如果这段代码能够正确并发的话，最后输出的结果应该是200000。读者运行完这段代码之后，并不会获得期望的结果，而且会发现每次运行程序，输出的结果都不一样，都是一个小于200000的数字，这是为什么呢？
问题就出现在自增运算“race++”之中，我们用Javap反编译这段代码后发现只有一行代码的increase()方法在Class文件中是由4条字节码指令构成的，从字节码层面上很容易就分析出并发失败的原因了：当getstatic指令把race的值取到操作栈顶时，volatile关键字保证了race的值在此时是正确的，但是在执行iconst_1、iadd这些指令的时候，其他线程可能已经把race的值加大了，而在操作栈顶的值就变成了过期的数据，所以putstatic指令执行后可能把较小的race值同步会主存之中。

由于volatile变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁（使用synchronized或java.util.concurrent中的原子类）来保证原子性。
- 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。
- 变量不需要与其他的状态变量共同参与不变约束。

### 禁止指令重排
在程序运行时，为了提升指令的执行效率，编译器重新排序，达到最佳效果。 
指令的重排序分为编译期重排和运行期重排。
编译期重排是编译器依据对上下文的分析，对指令进行重排序，使其更适合于CPU的并行执行。
编译器重排序：

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122925.png)

CPU只读一次的x和y值。不需反复读取寄存器来交替x和y值。编译器的重排序是为了更加高效的使用处理器。

**编译器重排序时会考虑指令的依赖性：**
1. 数据依赖性
如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分为下列3种类型，这3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122934.png)

2. 控制依赖性
flag变量是个标记，用来标识变量a是否已被写入，在use方法中变量i依赖if (flag)的判断，这里就叫控制依赖，如果发生了重排序，结果就不对了。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122943.png)

由于上面两种依赖性的存在，从而产生了as-if-serial 语义
为了不管如何重排序，都必须保证代码在单线程下的运行正确，连单线程下都无法正确，更不用讨论多线程并发的情况，所以就提出了一个as-if-serial的概念
s-if-serial 语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作依然可能被编译器和处理器重排序。
as-if-serial 并没有禁止存在控制依赖的指令进行重排序，因为控制依赖会降低流水线的并行度，所以处理器层面在处理条件分支时会采用猜测执行/流水线冒险来对分支指令进行预测执行，并在执行完成后对分支条件进行检查和校验冒险结果。所以无论是否重排序，处理器都会对其正确性进行检验。

而对于处理器执行指令时，为了使流水线的效率最大化，也会动态的根据依赖部件的效能对指令进行进一步的重排序。所以代码顺序并不是真正的执行顺序，只要有空间提高性能，CPU和编译器可以进行各种优化。

这种以提升执行效率为目的的重排序可能会带来意想不到的后果。为了在必要的时候避免重排序的发生，硬件为我们提供了**内存屏障机制**， 而volatile就是采用这个内存屏障机制来避免指令重排。

volatile变量在写操作之后会插入一个write屏障，在读操作之前会插入一个load屏障。一个类的final字段会在初始化后插入一个write屏障，来确保final字段在构造函数初始化完成并可被使用时可见。

# volatile和缓存一致性协议 mesi的关系
首先强调一点，volatile和mesi这两个东西没有半点关系。mesi是缓存一致性的一种实现手段，多核CPU为了保证缓存数据的一致性，通常有两种实现手段，一种是总线锁，另一种是缓存锁。总线锁性能消耗大，缓存锁则一般通过缓存一致性来实现。因此我们知道mesi是CPU硬件级别的。 volatile是JAVA的一种关键字，实现了两个功能： 1.可见性 2.禁止乱序。 禁止乱序，在JVM层面使用内存屏障来实现，汇编级别通过lock #指令来实现。不管cpu有没有mesi协议，用了volatile，JVM都会保证可见性，只不过实现方式是不一样的

问题：既然CPU有了MESI协议可以保证cache的一致性，那么为什么还需要volatile这个关键词来保证可见性(内存屏障)？或者是只有加了volatile的变量在多核cpu执行的时候才会触发缓存一致性协议？

答案是：还是有用的，就算在实现了mesi的cpu上，volatile一样不可或缺。除了禁止指令重排序的作用外，由于mesi只是保证了L1-3的cache之间的可见性，而cpu的并不是直接把数据写入L1 cache的，中间还可能有store buffer。有些arm和power架构的cpu还可能有load buffer或者invalid queue等等。因此，有MESI协议远远不够。而volatile规范保证了对它修饰的变量的写指令会使得当前cpu所有缓存写到被mesi保证可见性的L1-3cache中,然后触发MESI。（具体的实现，以X86体系为例，volatile会被JVM生成带lock前缀的指令）。

# 主备 主从 主主模式
单点故障的情况不可避免，而且单副本的存储方案早已无法满足业务的可靠性要求，因此一般情况下我们至少也会上个双机存储架构。凡事最好有个plan B。
## 主备
主：主机，备：备机。
主机的意思当然是以它为主了，读写都是主机上，而备机呢就是备用，默默的在背后吸收主机的数据，时刻待命着等待主机挂了之后取而代之(没这么坏哈哈)。因此在主机还活着的情况下，备机的唯一使命就是同步主机的数据，不对外提供服务。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409122953.png)

优点：简单，主备之间只有数据同步，不需要考虑别的情况。就很简单的配置一下，再搞一台服务器就能组成主备架构了。
缺点：备机等于就拿来备份，浪费了备机这台服务器的资源。上面说的不考虑别的情况指的是主机和备机它们两之间就只要复制数据，但是有些情况我们人还是得考虑的：主机挂了如何让备机上。
有三种选择
- 人工切换。人工切换时效性不高，出了事情首先你得开机，登录远程一阵啪啪得好几分钟或者万一你在LOL，黑铁晋级青铜最后一把努力了几个月即将晋升倔强青铜的一刻！是吧。还要万一在深夜或者说....是吧。
- 引入中间件。例如ZooKeeper、keepalived。就跟好多房东把房子委托给中介一样，这中间件就是个中介。全权由中介来打理主机和备机，它会根据机子状态来判别这时候是不是该备机上了。(建议)
- 主机备机之间状态传输(咱不找中介了，自己来打理)，啥意思呢？就是除数据同步，主备之间还要有个状态传输过程，来让备机只要现在主机过得好不好，可以是主机主动推送它的状态给备机，或者是备机去索要状态。当状态拿不到或者不对的时候就开始主备切换。但是可能传输出现了波动啥的，导致备机误判了，然后备机升级为主机，这样就两主机了(下面会说主主的问题)。

## 主从
主：主机，从：从机
从机和备机的区别在于它得除了同步数据之外还得干活，对外提供读的操作，你可以理解为它是仆从。但是仆从和备机一样也有翻身做主人的一天，所以它也在默默的等待着主机挂了，取而代之。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409123002.png)

优点：充分利用了资源，嘿嘿不想备机这么爽了，还得出来干活，对外提供读操作。而且在主机挂了的时候，如果没任命新机主之前，读操作还是能用的。
缺点：
- 客户端需要多个判断，也就是不同操作需要发放给不同服务器，我上图主机提供读写，有时候读写分离了，主机就提供写。
- 主从延迟，读操作分配给从库，就会存在数据同步的延迟问题，比如某个人注册了账号之后，登录走的是从机，这时候数据还未从主机同步过来，那可不让人很难受了。有关主从延迟问题的一些解决办法
- 和主备一样的切换问题。(参考主备)

## 主主
主主就是两台都是主机。同时对外提供读写操作。客户端任意访问提供的一台。

![](https://raw.githubusercontent.com/NaisWang/images/master/20220409123014.png)

优点：主主的好处就是可以把写操作也分担一下，但是问题恰恰就出在写操作上，导致主主的架构有很大的局限性。
缺点：例如主机A有个注册的插入操作，生成的id是50，同一时刻主机B也有个插入操作生成的id也是50。然后它们之间的数据同步了，你说是谁覆盖谁呢？谁覆盖谁都不对！
因此主主只适用于可以双向复制，覆盖的数据(例如用户登录生成的token)。但是我们平日里绝大部分的数据都不允许。
