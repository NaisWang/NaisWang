# google搜索技巧
1. `site`：在关键词后加上site:指定的网站 eg:`后浪 site:www.bilibili.com`
2. `准确搜索`: 给关键词加上英文双引号 eg:`“人工智能算法”`
3. `排除关键词`：在搜索内容后面加上空格减号需要排除的关键词 eg:`苹果 -iPhone -iPad`
4. `用OR逻辑搜索`：用大写的OR和空格隔开关键词 eg: `百度 OR 谷歌`
5. `模糊搜索`：用*代替文字或单词 eg:`study * home`
6. `filetype`：在关键词后加上filetype:文件类型 eg:`高等数学 filetype:pdf`
7. `inurl/allinurl`：在关键词后加上inurl:需要筛选的url关键字，allinurl必须同时包含关键词 eg:`肖生克的救赎 inurl:movie film`
8. `intitle/allintitle`：在关键词后加上intitle:需要筛选的title关键字，allintitle必须同时包含关键词 eg:`machine learning intitle:stanford mit`
9. `define`：通过define:关键词得到准确定义 eg:`define:internet`

组合示例：`machine learning -vision -drive site:stanford.edu filetype:pdf`


# 搜索引擎工作原理
搜索引擎的工作过程大体可以分为三个阶段：
1. 对网页进行抓取建库
搜索引擎蜘蛛通过抓取页面上的链接访问其他网页，将获得的HTML代码存入数据库
2. 预处理
索引程序对抓取来的页面数据进行文字提取、中文分词、索引等处理，为后面排名程序使用时做准备。
3. 给搜索结果进行排名
用户输入关键词后，排名程序调用索引库数据，计算数据和关键词的相关性，然后按照一定格式生成搜索结果页面。

以上阶段用到了大概三个程序，蜘蛛、索引程序、排名程序

## 对网页进行爬行、抓取、建库
如果我们要从一个页面进入另一个页面，我们需要在页面上点击这个超链接跳转到新的页面，这个链接指向另一个网页，相当于这个网页的入口
![](https://gitee.com/NaisWang/images/raw/master/img/20211101222220.png)
或者如果我们知道这个网页的url地址，就算我们没有在页面上看到链接到该网页的可点击的超链接，也可以通过在地址栏输入url地址转到该页面
![](https://gitee.com/NaisWang/images/raw/master/img/20211101222303.png)

当我们在一个网站发布了自己的文章，这篇文章会产生一个新的独一无二的url地址，当人们点击这个地址，它不会跳转到其他人写的文章页面，而是你写的特定的那一篇。
可以看出，所有页面，都会产生一个url地址使我们可以访问它。

整个互联网是由相互链接的页面组成的，如果一个网页，没有任何一个页面链接它，我们也不知道这个网页的地址，就算这个页面真实存在，它也会像一个孤岛一样，我们无法访问到这个页面。

日常生活中我们有多个搜索引擎可以使用，比如百度、谷歌、搜狗、bing等。
不同的搜索引擎就算查询同一个内容返回出来的结果都不一样，这是因为各个公司给内容进行排序的计算方式都是不一样的，哪个页面该排到第一页/哪个网页该排到最后一页/哪个网页根本就不应该展示出来都是有他们公司自己的评判标准，这些排名算法具体的内容基本都不会对外公开，避免被其他公司搜索引擎公司知道，因为排名算法是每个搜索引擎公司的核心竞争力。

**为什么排名算法是每个搜索引擎公司的核心竞争力?**
一般人们都会看哪个搜索引擎搜索出来的结果更符合TA自己的需求（相关性更高）就会选择长期使用哪一个。
比如，你在搜索输入框里输入【空调】两个字想查询关于空调的信息，结果搜索结果页给你返回的内容第一页竟然是一些电视机/马桶/衣柜之类的销售链接，这样相关性不高的网页越多，对你的使用体验就越差，最好的体验是，你搜索【空调】后，返回的页面里全是关于空调的信息，这样节约了你获取信息的时间成本，使你更方便的获取想要的资讯。
而排名算法就是为了让返回的结果尽量符合用户查询的内容的一种算法，他会对网页进行排名，把觉得对用户最有价值的网页排在前面，比如第一页第一个，用户能最快的看到这个网页，把相关性较差不重要的网页排在后面。把那些没有用的没有价值的页面直接不展示出来，经过对这些网页的排序，让用户尽量在只看第一页的情况下就能找到自己想要的资讯，解决掉自己的问题。
所以搜索引擎公司只要能对网页进行合理的排序，带给用户最大的方便，让用户感觉到返回的内容都很精准，正好是他们想要的内容，那么用户就会持续使用这个搜索引擎，所以如何对这些网页进行排序的计算方式就是每个搜索引擎公司的公司机密了。

爬行和抓取搜索引擎工作的第一步，目的是完成数据收集的任务。
当用户在搜索框输入想查询的内容后，所有展示出来的网页都是需要先经过搜索引擎的收集才能展示出来的，只有收集了，才能通过分析网页中的内容，对这些网页的价值和相关性进行一个判断，经过对网页的排序之后再返回给用户，用户在搜索结果页上看到的所有网页，都是已经被搜索引擎收集进数据库中的网页。
而那些互联网上没有被搜索引擎收集到的网页（搜索引擎不是什么网页都会放进数据库，每个搜索引擎都有自己的一个标准，就是什么样的网页才会被收集到数据库中。就像人类吃东西一样，只吃自己认为该吃的食物），就变成了永远无法访问的孤魂野鬼。
注：网页和网站的区别需要注意，蜘蛛在判断需不需要收集进索引数据库的是以网页为单位的。
比如整个淘宝是一个网站，但是淘宝中某一个商品的详情页面才算网页，还比如你现在所看的这篇文章所在的这一个页面才算网页。
所以，整个淘宝网站的页面那么多，蜘蛛在收集网页时，就算收集了A商品那页详情页面，但不代表B商品详情页面也被收集进索引数据库了，蜘蛛会对每个页面进行评判，只有该页面到达他的标准了，认为可以收集到索引数据库里了，才会把这个页面添加进去，而不是它认为淘宝这个网站很有价值，就把整个网站里所有页面全部收录进去了，SEO里有个概念叫做收录率，指的是页面的收录率，而不是网站的收录率。

### 蜘蛛
搜索引擎用来爬行和访问页面的程序叫做蜘蛛/爬虫（spider），或机器人（bot）。
蜘蛛访问网站页面的流程和人们在浏览器上访问页面的流程差不多，蜘蛛访问页面时，会发出页面访问请求，服务器会返回HTML代码，蜘蛛把收到的HTML代码存入原始页面数据库。

互联网上的页面这么多，为了提高爬行和抓取的速度，搜索引擎会同时使用多个蜘蛛对页面进行爬行。

理论上来说，互联网上的所有页面（这里指的是通过超链接互联链接在一起的页面，而不是那种虽然这个页面存在，但是没有任何网页用超链接指向他），蜘蛛都可以沿着页面上的超链接将所有页面爬行一遍，但是蜘蛛不会这么做，蜘蛛的时间有限，它会用效率最高的方式找到互联网上它觉得最有价值的网页。
如果一个网站的页面普遍质量较低，蜘蛛就会认为这是一个低质网站，让用户阅读这类没有价值的网页是没有必要的，对于这类网页，它会减少爬行的频率，将重点放在其他质量更高的网站，去其他更有价值的网站上收集网页存入数据库。

但是蜘蛛不是全能，它也有评判错误的时候，比如蜘蛛不会收集空短页面，也就是内容空洞毫无营养，主体内容又短的页面，蜘蛛不会浪费数据库的空间放入这些网页。

但是一个页面究竟是不是真正的无价值网页，蜘蛛判断成功的正确率并不是100%，就像一个登录验证网页，确实没有比那些传递知识的网页更有价值，但是这是大部分网站一个必不可少的一个页面，严格来说并不是低质页面，但是如果蜘蛛爬行到了这个页面，它并不理解人类眼中的验证页面是一个怎样的存在，他只觉得，内容短，没什么丰富的内容，好，那就是无价值的网页了，它就像有一个专门用来记录的小本本一样，嗯，A网站，有一个低质页面，给这个网站评价好还是不好我还要在考虑一下，如果这样的低质页面多了，蜘蛛就会觉得你的网站整体质量较低，慢慢的就不爱到你网站上抓取网页了。

##### robots协议
所以为了避免上述情况，不让蜘蛛抓取这些网页是最好的办法，我们可以在项目根目录创建一个txt文件，这个文件叫什么是有约定俗成的，文件名必须为 robots.txt，我们在文件里面规定好蜘蛛可以爬行/不能爬行哪些网页就行。这被称为robots协议

当蜘蛛访问任何一个网站的时候，第一件事就是先访问这个网站根目录下的robots.txt文件，如果文件里说了禁止让蜘蛛抓取XX文件/XX目录，蜘蛛就会按照文件里规定的那样，只抓取可以抓取的页面。

例如：taobao.com就在robots.txt中明确规定了禁止百度爬虫爬取
![](https://gitee.com/NaisWang/images/raw/master/img/20211101225253.png)
我们通过百度搜索淘宝，出现如下画面，说明百度爬虫遵守了淘宝的robots协议
![](https://gitee.com/NaisWang/images/raw/master/img/20211101225519.png)

### 蜘蛛的分类
每个搜索引擎公司都有自己的蜘蛛，这些蜘蛛喜好不一，喜欢抓取什么类型的网页要看他们的主人，也就是要看搜索引擎公司他们的想法，虽然这些蜘蛛统称为蜘蛛，但他们属于不同的主人，当然名字肯定是不一样的。

在日志文件中可以看到有哪些公司的蜘蛛来访问过网站（user-agent那个）
蜘蛛主要分为下面几个：
- 百度蜘蛛 Baiduspider
- 谷歌蜘蛛 Googlebot
- 有道蜘蛛 YodaoBot
理论上来说，随便找一个页面，顺着这个页面，蜘蛛可以将互联网上所有的页面都爬一遍
实际上这样确实是可行的（除去那些没有被任何一个网页所指向的页面），而蜘蛛是如何做到的呢？

比如，蜘蛛先从A页面开始，它爬行到A页面上，它可以获取到A页面中所有的超链接，蜘蛛再顺着这个链接进入到链接所指向的页面，再获取到这个页面上所有的超链接进行爬行抓取，这样一来，所有用超链接所关联上的网页便可以被蜘蛛都爬行一遍。

蜘蛛在爬行时，也是有自己的爬行策略的，就像吃西瓜，把整个西瓜切一半切成一个半圆体，我们选择吃西瓜的方式可以深度优先，随便从中间还是边缘开始吃都行，比如先从中间吃，西瓜中间底下全部挖干净了再围着中间的圆圈慢慢往外面扩散。或者广度优先，从中间或者是边缘随便哪里开始都行，把表面一层挖完了西瓜再进行下一个深度的挖取。
![](https://gitee.com/NaisWang/images/raw/master/img/20211101222851.png)

### 蜘蛛的爬行策略
蜘蛛的爬行策略和挖西瓜一样，都是两种方式
- 深度优先
- 广度优先

深度优先如下图
![](https://gitee.com/NaisWang/images/raw/master/img/20211101223001.png)
蜘蛛先从A页面开始爬行，发现该页面总共有3个超链接，A1、B1、XX，蜘蛛选择先从A1页面爬行下去，它在A1页面发现了一个唯一的超链接A2，便沿着A2向下，以此类推，等爬到最底下，也就是A4页面，A4整个页面上没有任何超链接，再也无法往下爬行了，它便返回到B1开始爬行，这就是深度优先。

广度优先如下
![](https://gitee.com/NaisWang/images/raw/master/img/20211101223024.png)

这次这是一个利用广度优先策略的蜘蛛，它先从A页面出发，现在A页面有3个链接，A1、B1、C1，它会先把A1、B1、C1先爬一遍，也就是第一层发现的超链接全部爬行完，然后再进入第二层，也就是A1页面。把A1页面中所有的超链接全部爬行一遍，保证广度上全部链接是都完成爬行了的。
无论是深度优先还是广度优先，蜘蛛都可以通过这两个策略完成对整个互联网页面的爬行。
当然，由于蜘蛛的带宽资源和时间有限的问题，蜘蛛不会选择爬完所有页面，它实际收集到的页面知识互联网的一小部分，在条件限制的情况下，蜘蛛通常会深度优先和广度优先混合使用，广度优先保证了尽可能照顾到多的网站，而广度优先保证了尽可能照顾到一部分网站中的内页。

### 吸引蜘蛛抓取页面
可以看出，在实际情况中，蜘蛛不会爬行、抓取互联网上所有的页面，既然如此，蜘蛛所要做的就是尽量抓取重要页面，而SEO人员要做的，就是吸引蜘蛛的注意，让蜘蛛更多的抓取自己家网站的页面。

对于蜘蛛来说，页面拥有哪些特征会被看作是重要页面呢，主要有以下这几方面因素：
1. 网站和页面权重
质量高，资格老的网站被认为权重较高，这种网站上页面的爬行深度也会比较高，所以这种网站网页被收录的机会会更多。

2. 页面更新度
如A网页的数据之前在蜘蛛爬行后已经被保存在数据库中了，当蜘蛛第二次爬行A网页时，会将A网页此时的数据和数据库中的数据进行对比，如果蜘蛛发现A网页的内容更新了，就会认为这个网页更新频率多，蜘蛛抓取这个页面的频率也会更加频繁，如果页面和上次储存的数据完全一样，就说明页面是没更新，蜘蛛就会减少自己爬行该页面的频率。

3. 高质量的外链
张三是班上公认的人品好为人公正的学霸，李四是班上惹人讨厌最爱撒谎的学生，张三给大家说王五这个人真的很聪明为人也很善良，其他同学都会认为王五肯定是这样，李四给其他同学说王五这个人很好，其他同学基本不会相信李四的鬼话。
同样一句话，从不同人的嘴里说出来，造成的结果、影响都不一样。
链接的引用也是这样，比如在一个蜘蛛认为的高质量页面中，页面在最后引用了一个链接，指向你的页面，那么这个高质量页面的引用，在蜘蛛判断你的网页是否是高质量网页时，也会产生一定的影响，被高质量网页引用的多了（超级多的大佬夸你人好），那么蜘蛛在判断你页面时产生的影响也就更大（同学也觉得你就是人好）。

4. 与首页的距离
一般来说自己网站被其他网站引用最多的页面就是首页，所以它的权重相比来说是最高的，比如A页面是A网站的首页，可以得出的结论是，离A网页更进的页面，页面权重也容易更高，比如A页面上的超链接更容易被蜘蛛爬行，更容易获得蜘蛛的抓取，那些没被蜘蛛发现的网页，权重自然就是0。
还有一点比较重要的是，蜘蛛在爬行页面时会进行一定程度的复制检测，也就是当前被爬行的页面的内容，是否和已经保存的数据有重合（当页面内容为转载/不当抄袭行为时就会被蜘蛛检测出来），如果一个权重很低的网站上有大量转载/抄袭行为，蜘蛛很可能不会再继续爬行。
之所以要这么做也是为了用户的体验，如果没有这些去重步骤，当用户想要搜索一些内容时，发现返回的结果全都是一模一样的内容，会大大影响用户的体验，最后导致的结果就是这个搜索引擎绝对不会有人再用了，所以为了用户使用的便利，也是为了自己公司的正常发展。

### 地址库
互联网上的网页这么多，为了避免重复爬行和抓取网页，搜索引擎会建立地址库，一个是用来记录已经被发现但还没有抓取的页面，一个是已经被抓取过的页面。

待访问地址库（已经发现但没有抓取）中的地址来源于下面几种方式：
1. 人工录入的地址
2. 蜘蛛抓取页面后，从HTML代码中获取新的链接地址，和这两个地址库中的数据进行对比，如果没有，就把地址存入待访问地址库。
3. 站长（网站负责人）提交上去的想让搜索引擎抓取的页面。（一般这种效果不大）

蜘蛛按照重要性从待访问地址库中提取URL，访问并抓取页面，然后把这个URL地址从待访问地址库中删除，放进已访问地址库中。

### 文件存储
蜘蛛会将抓取的数据存入原始页面数据库。
存入的数据和服务器返回给蜘蛛的HTML内容是一样的，每个页面存在数据库里时都有自己的一个独一无二的文件编号。

## 预处理
我们去商场买菜时，会看到蔬菜保险柜里的这些蔬菜被摆放的整整齐齐，这里举的例子是那些用保鲜膜包好有经过包装的蔬菜。
最后呈现在顾客面前的是包装完好，按照不同的分类摆放有序，顾客一眼就能很清楚的看到每个区域分别是什么蔬菜。
在最终完成这个结果之前，整个流程大概也是三个步骤：
1. 选出可以售卖的蔬菜
从一堆蔬菜中，选出可以拿去售卖的蔬菜。
2. 预处理
此时你面前摆放的就是全部可以拿去售卖的蔬菜了，但是如果，今天就要把这些蔬菜放到蔬菜保险柜中的话，你今天才开始对这些蔬菜进行整理会浪费大量的时间（给蔬菜进行包装等），说不定顾客来了蔬菜还没整理好。
所以你的解决方法是，提前将这些可以拿去售卖的蔬菜提前包装好，存放在仓库里，等保险柜中的蔬菜缺少了需要补货时，花个几分钟时间跑去仓库把蔬菜拿出来再摆放再货架上就行了。（我猜的，具体商场里的流程是怎么样的我也不知道，为了方便后续的理解用生活上的例子进行说明效果会更好）
3. 摆放上保险柜
也就是上面最后一段内容那样，当需要补货时，从仓库里拿出包装好的蔬菜，按照蔬菜的类别摆放到合适的位置就可以了，这个就是最后的排序步骤。

回到搜索引擎的工作流程中，这个预处理的步骤就和上面商场预处理步骤的作用一样。

当蜘蛛完成数据收集后，就会进入到这个步骤。

蜘蛛所完成的工作，就是在收集了数据后将数据（HTML）存入原始页面数据库。
而这些数据，不是用户在搜索后，直接用来进行排序并展示在搜索结果页的数据。

原始页面数据库中的页面数量都是在数万亿级别以上，如果在用户搜索后对原始页面数据库中的数据进行实时排序，让排名程序（每个步骤所使用的程序不一样，收集数据的程序叫蜘蛛，排名时所用的程序是排名程序）分析每个页面数据与用户想搜索的内容的相关性，计算量太大，会浪费太多时间，不可能在一两秒内返回排名结果。

因此，我们需要先将原始页面数据库中的数据进行预处理，为最后的排名做好准备。

### 提取文字
我们存入原始页面数据库中的，是HTML代码，而HTML代码中，不仅有用户在页面上直接可以看到的文字内容，还有其他例如js，AJAX等这类搜索引擎无法用于排名的内容。
首先要做的，就是从HTML文件中去除这些无法解析的内容，提取出可以进行排名处理步骤的文字内容
比如下面这段代码
```html
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="这是一个描述内容"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no">
    <link rel="icon" type="image/png" sizes="32x32" href="//static001.infoq.cn/static/infoq/www/img/InfoQ-share-icon2.jpg">
    <title>软件工程师需要了解的搜索引擎知识</title>
    <script type="text/javascript" src="//res.wx.qq.com/open/js/jweixin-1.4.0.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          displayMath: [ ["$$","$$"] ],
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code', 'a']
        }
      });
      MathJax.Hub.Register.MessageHook("End Process", function (message) {
        var eve = new Event('mathjaxfini')
        window.dispatchEvent(eve)
      })
    </script>
</head>
<body>
<div id="app">hi</div>
<img alt="Google" src="/images/test.png" 
</body>
</html>
```
可以看出整个HTML中，真正属于文字内容的信息只有两句
```
这是一个描述内容
软件工程师需要了解的搜索引擎知识
hi
Google
```
搜索引擎最终提取出来的信息就是这四句，用于排名的文字也是这四句。
可以提取出来的文字内容大概就是，Meta标签中的文字、img标签alt属性中的文字、Flash文件的替代文字、链接锚文字等。

#### 中文分词
分词是中文搜索引擎特有的步骤，搜索引擎存储/处理页面/用户搜索时都是以词为基础的。
```
I'm fine, and you?
```
中文和英文等语言单词不同，在使用英文时各个单词会有空格分隔，搜索引擎可以直接把每一个句子划分为多个英文单词的集合。而对中文来说，词汇和词汇之间是没有任何分隔符可以对各词汇进行分隔的。
```
比如这句话里的词就是连接在一起的
```
对于这种情况，搜索引擎首先需要分辨哪几个字组成一个词，如 我喜欢吃【水果】，或者哪些字本身就是一个词，如 这里有【水】，
再如下面这句话
```
你好，这是一篇关于搜索引擎的文章
```
搜索引擎会将这一段文字拆解成一个个词汇，大概如下
```
你好
这是
一篇
关于
搜索引擎
的
文章
```
搜索引擎将这段文字拆解成了7个词汇（我瞎猜的，具体多少个我也不知道，每个搜索引擎分词的方法都不一样）

中文分词的方法基本上有两种：
- 基于词典匹配
- 基于统计

1. 基于词典匹配
将需要分析的一段汉字与一个时间创建好的词典中的词条进行匹配，如果在这段汉字中扫描到词典中已有的词条则匹配成功。
这种匹配方式最简单，但匹配的正确程序取决于这个词典的完整性和更新情况。

2. 基于统计
一般是通过机器学习完成，通过对海量网页上的文字样本进行分析，计算出字与字相邻出现的统计概率，几个字相邻出现越多，就越可能形成一个词。
这种优势是对新出现的词反应更快速。

实际使用中的分词系统都是两种方法同时混合使用。

#### 去停止词
不管是英文还是中文，页面中都会有一些出现频率很高的&对内容没有任何影响的词，如中文的【的】、【啊】、【哈】之类，这些词被称为停止词。
英文中常见的停止词有`[the]/[a]/[an]`等。

搜索引擎会去掉这些停止词，使数据主题更突出，减少无谓的计算量。

#### 去掉噪声词
大部分页面里有这么一部分内容对页面主题没什么贡献，比如A页面的内容是一篇关于SEO优化的文章，关键词是SEO，但是除了讲解SEO这个内容的主体内容外，共同组成这个页面的还有例如页眉，页脚，广告等区域
![](https://gitee.com/NaisWang/images/raw/master/img/20211101224527.png)

在这些部分出现的词语可能和页面内容本身的关键词并不相关。
比如导航栏中如何出现【历史】这个词，导航栏上想要表达的实际是历史记录之类的意思，搜索引擎可能会把他误以为是XX国家历史，XX时代历史之类这种层面的【历史】，搜索引擎所理解的和页面本身内容想表达的完全不相关，所以这些区域都属于噪声，在搜索引擎分析一个页面的时候，它们只会对页面主题起到分散作用。
搜索引擎的排名程序在对数据进行排名时不能参考这些噪声内容，我们在预处理阶段就需要把这些噪声时别出来并消除他们。
消除噪声的方法是根据HTML的标签对页面进行分块，如页眉是header标签，页脚是footer标签等等，去除掉这些区域后，剩下的才是页面主体内容。

#### 引导搜索引擎分词
当你搜索`2021大连续返乡`时，你会发现百度会搜索出含有“连续”/“返乡”字样的文章。而不是搜索出含有“大连”字样的文章
![](https://gitee.com/NaisWang/images/raw/master/img/20211102081211.png)
这是因为百度将“大连续”中的“连续”分成了一个词，而不是将`大连`分成一个词，所以会搜索出含有“连续”/“返乡”字样的文章。而不是搜索出含有“大连”字样的文章。
为了让百度搜索出含有“大连”字样的文章，我们需要引导百度将“大连”分成一个词，具体方法是加空格，如：`2021大连 续返乡`, 搜索的结果如下：
![](https://gitee.com/NaisWang/images/raw/master/img/20211102081935.png)

>总结：在你搜索很长的句子的时候，你可以适当的通过加空格去引导搜索引擎按照你的想法给你分词，因为分词准确了，才更有可能得到你想要搜索的东西

### 去重
也就是去掉重复的网页，同一篇文章经常会重复在不同网站/同一个网站的不同网址上。为了用户的体验，去重步骤是必须的，搜索引擎会对页面进行识别&删除重复内容，这个过程称为蛆虫和。
去重的方法是先从页面主体内容中选取最有代表性的一部分关键词（经常是出现频率最高的关键词，由于之前已经有了去停止词的步骤，因此在这时出现频率最高的关键词可能就真的是整个页面的关键词了），然后计算这些关键词的数字指纹。
通常我们在页面中选取10个关键词就可以达到比较高的计算准确性了。
典型的指纹计算方法如MD5算法（信息摘要算法第五版）。这类指纹算法的特点是，输入（也就是上面提取出来的关键词）只要有任何微小的变化，都会导致计算出的指纹有很大差距。
比如我们用两个数相乘，第一组和第二组的不同仅仅是第一个数字 0.001 的差别，最终生成的结果却千差万别。
![](https://gitee.com/NaisWang/images/raw/master/img/20211101224610.png)

了解了搜索引擎的去重算法后，就会发现那些在文章发布者眼里的原创内容实际对搜索引擎来说就是非原创，比如简单的增加/删除【的】【地】等这些去停止词、调换段落顺序、混合不同文章等操作，在搜索引擎进行去重算法后，都会被判断为非原创内容，因为这些操作并不会改变文章的关键词。
（比如我写的这篇笔记里的一些段落就是‘借鉴’了一下，我是从书里看的不是在网页上直接浏览的，如果搜索引擎在对我这篇文章进行文字提取、分词、消噪、去重后，发现剩下的关键词和已收录的某个网页数据的内容都匹配上了，就会认为我是伪原创甚至非原创，最终影响的就是我这篇文章在搜索引擎工作原理这个关键词上的排名）

### 正向索引
正向索引可以简称为索引。
经过上述各步骤（提取、分词、消噪、去重）后，搜索引擎最终得到的就是独特的、能反映页面主体内容的、以词为单位的内容。

接下来由搜索引擎的索引程序提取关键词，按照分词程序划分好的词，把页面转换为一个由关键词组成的集合，同时还需要记录每一个关键词在页面上的出现频率、出现次数、格式（如是出现在标题标签、黑体、h标签、还是锚文字等）、位置（如页面第一段文字等）。

搜索引擎的索引程序会将页面和关键词形成的词表结构存储进索引库。
简化的索引词表形式如图
![](https://gitee.com/NaisWang/images/raw/master/img/20211101224644.png)

每个文件都对应一个文件ID，文件内容被表示成一串关键词的集合。
实际上在搜索引擎索引库中，关键词也已经转换为关键词ID，这样的数据结构被称为正向索引。

### 倒排索引
正向索引不能直接用于排名，假设用户搜索关键词【2】，如果只存在正向索引，排名程序需要扫描所有索引库中的文件，找出包含关键词【2】的文件，再进行相关性计算。
这样的计算量无法满足实时返回排名结果的要求。

我们可以提前对所有关键词进行分类，搜索引擎会将正向索引数据库重新构造为倒排索引，把文件对应到关键词的映射转换为关键词到文件的映射，如下图
![](https://gitee.com/NaisWang/images/raw/master/img/20211101224710.png)

在倒排索引中关键词是主键，每个关键词都对应着一系列文件，比如上图第一排右侧显示出来的文件，都是包含了关键词1的文件。
这样当用户搜索某个关键词时，排序程序在倒排索引中定位到这个关键词，就可以马上找出所有包含这个关键词的文件。

## 给搜索结果进行排名
经过前面的蜘蛛抓取页面，对数据预处理&索引程序计算得到倒排索引后，搜索引擎就准备好可以随时处理用户搜索了。
用户在搜索框输入想要查询的内容后，排名程序调用索引库的数据，计算排名后将内容展示在搜索结果页中。

### 搜索词处理
搜索引擎接收到用户输入的搜索词后，需要对搜索词做一些处理，然后才进入排名过程。
搜索词处理过程包括如下几个方面：
1. 中文分词
和之前预处理步骤中的分词流程一样，搜索词也必须进行中文分词，将查询字符串转换为以词为单位的关键词组合。
分词原理和页面分词时相同。
2. 去停止词
同上。
3. 指令处理
上面两个步骤完成后，搜索引擎对剩下的内容的默认处理方式是在关键词之间使用【与】逻辑。
比如用户在搜索框中输入【减肥的方法】，经过分词和去停止词后，剩下的关键词为【减肥】、【方法】，搜索引擎排序时默认认为，用户想要查询的内容既包含【减肥】，也！注意这个也！！！也包含【方法】！
只包含【减肥】不包含【方法】，或者只包含【方法】不包含【减肥】的页面，都会被认为是不符合搜索条件的。

### 文件匹配
搜索词经过上面的处理后，搜索引擎得到的是以词为单位的关键词集合。
进入的下一个阶段-文件匹配阶段，就是找出含有所有关键词的文件。
在索引部分提到的倒排索引使得文件匹配能够快速完成，如下图
![](https://gitee.com/NaisWang/images/raw/master/img/20211101224851.png)

假设用户搜索【关键词2 关键词7】，排名程序只要在倒排索引中找到【关键词2】和【关键词7】这两个词，就能找到分别含有这两个词的所有页面文件。
经过简单计算就能找出既包含【关键词2】，也包含【关键词7】的所有页面：【文件1】和【文件6】。

### 初始子集的选择
找到包含所有关键词的匹配文件后，还不能对这些文件进行相关性计算，因为在实际情况中，找到的文件经常会有几十万几百万，甚至上千万个。要对这么多文件实时进行相关性计算，需要的时间还是挺长的。
实际上大部分用户只喜欢查看前面两页，也就是前20个结果，后面的真的是懒都懒得翻！

由于所有匹配文件都已经具备了最基本的相关性（这些文件都包含所有查询关键词），搜索引擎会先筛选出1000个页面权重较高的一个文件，通过对权重的筛选初始化一个子集，再对这个子集中的页面进行相关性计算。

### 相关性计算
用权重选出初始子集之后，就是对子集中的页面计算关键词相关性的步骤了。

计算相关性是排名过程中最重要的一步。

影响相关性的主要因素包括如下几个方面：
1. 关键词常用程度。
经过分词后的多个关键词，对整个搜索字符串的意义贡献并不相同。
越常用的词对搜索词的意义贡献越小，越不常用的词对搜索词的意义贡献越大。举个例子，假设用户输入的搜索词是“我们冥王星”。“我们”这个词常用程度非常高，在很多页面上会出现，它对“我们冥王星”这个搜索词的辨识程度和意义相关度贡献就很小。找出那些包含“我们”这个词的页面，对搜索排名相关性几乎没有什么影响，有太多页面包含“我们”这个词。
而“冥王星”这个词常用程度就比较低，对“我们冥王星”这个搜索词的意义贡献要大得多。那些包含“冥王星”这个词的页面，对“我们冥王星”这个搜索词会更为相关。
常用词的极致就是停止词，对页面意义完全没有影响。
所以搜索引擎对搜索词串中的关键词并不是一视同仁地处理，而是根据常用程度进行加权。不常用的词加权系数高，常用词加权系数低，排名算法对不常用的词给予更多关注。
我们假设A、B两个页面都各出现“我们”及“冥王星”两个词。但是“我们”这个词在A页面出现于普通文字中，“冥王星”这个词在A页面出现于标题标签中。B页面正相反，“我们”出现在标题标签中，而“冥王星”出现在普通文字中。那么针对“我们冥王星”这个搜索词，A页面将更相关。
2. 词频及密度。一般认为在没有关键词堆积的情况下，搜索词在页面中出现的次数多，密度越高，说明页面与搜索词越相关。当然这只是一个大致规律，实际情况未必如此，所以相关性计算还有其他因素。出现频率及密度只是因素的一部分，而且重要程度越来越低。
3. 关键词位置及形式。就像在索引部分中提到的，页面关键词出现的格式和位置都被记录在索引库中。关键词出现在比较重要的位置，如标题标签、黑体、H1等，说明页面与关键词越相关。这一部分就是页面SEO所要解决的。
4. 关键词距离。切分后的关键词完整匹配地出现，说明与搜索词最相关。比如搜索“减肥方法”时，页面上连续完整出现“减肥方法”四个字是最相关的。如果“减肥”和“方法”两个词没有连续匹配出现，出现的距离近一些，也被搜索引擎认为相关性稍微大一些。
5. 链接分析及页面权重。除了页面本身的因素，页面之间的链接和权重关系也影响关键词的相关性，其中最重要的是锚文字。页面有越多以搜索词为锚文字的导入链接，说明页面的相关性越强。
链接分析还包括了链接源页面本身的主题、锚文字周围的文字等。



